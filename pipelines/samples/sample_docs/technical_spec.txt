Cortex Data Pipeline Technical Specification
=============================================

Version: 0.1.0
Last Updated: 2025-01-11

1. OVERVIEW
-----------
The Cortex data pipeline implements a lakehouse architecture using Apache Iceberg
tables for metadata storage, MinIO for raw object storage, and DuckDB for query
processing. The pipeline is designed to run locally and scale to cloud deployment.

2. ARCHITECTURE COMPONENTS
--------------------------
2.1 Ingestion Layer
  - File drop connector (local filesystem)
  - Future: API connectors for GitHub, Notion, etc.
  - Content-addressable storage using SHA256 hashing
  - Automatic deduplication

2.2 Storage Layer
  - MinIO: S3-compatible object store for raw files
  - Apache Iceberg: Metadata tables with ACID guarantees
  - PostgreSQL: Iceberg catalog backend

2.3 Processing Layer
  - DuckDB: SQL query engine with Iceberg support
  - Dagster: Orchestration and scheduling
  - Sensors: Continuous monitoring of watch directories

2.4 Observability
  - Marquez: OpenLineage-based data lineage tracking
  - Structured logging: JSON format for all components
  - Dagster UI: Real-time monitoring of pipeline runs

3. DATA MODEL
-------------
3.1 Document Table (catalog.docs)
  - doc_id (PK): SHA256 hash of content
  - source_id: External identifier
  - mime: MIME type
  - size_bytes: File size
  - ingest_first_at: First ingestion timestamp
  - ingest_last_at: Last seen timestamp
  - Partitioned by: ingest_date (daily)

3.2 Version Table (catalog.doc_versions)
  - doc_id: Reference to docs table
  - run_id: Dagster run identifier
  - ingest_at: Timestamp
  - Enables time travel and reproducibility

3.3 Lineage Table (catalog.events_lineage)
  - Tracks data lineage per OpenLineage spec
  - Records input/output relationships
  - Supports impact analysis

4. OPERATIONAL REQUIREMENTS
----------------------------
4.1 Local Development
  - Docker Compose for all services
  - 8GB RAM minimum
  - 20GB disk space for data
  - Make targets for common operations

4.2 Performance
  - Ingestion: 100+ files/minute
  - Query latency: <100ms for metadata
  - Deduplication: O(1) via hash lookup

4.3 Data Quality
  - MIME type validation
  - File size limits (50MB default)
  - Character encoding detection
  - Checksum verification

5. FUTURE ENHANCEMENTS
----------------------
- Text extraction (PDF, HTML)
- Entity extraction with NLP
- Vector embeddings for semantic search
- Graph population (Neo4j)
- Cloud deployment automation
- Real-time streaming ingestion
